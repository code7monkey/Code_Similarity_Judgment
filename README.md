# GraphCodeBERT Code Similarity Analysis Project

This repository provides a **sample project for training and inference of a code similarity classification model** based on **GraphCodeBERT**.  
The original experimental code written in Jupyter notebooks has been refactored into a **modular Python package** to improve readability, reusability, and maintainability.

---

## ðŸ”¥ Project Objectives

- **Code preprocessing and pair generation**  
  Provide utilities such as `remove_extras` to remove comments and unnecessary whitespace from source code, and generate positive/negative code pairs for training.

- **GraphCodeBERT-based classifier**  
  Implement a code similarity prediction model by wrapping HuggingFaceâ€™s `AutoModelForSequenceClassification` with GraphCodeBERT as the backbone.

- **Modular training and inference pipeline**  
  Define training, validation, and inference routines in `src/trainer.py`, and execute them via top-level scripts that read YAML configuration files.

- **Easy experiment control**  
  Adjust data paths, hyperparameters, and model save/load settings through `configs/train.yaml` and `configs/submit.yaml` without modifying source code.

---

## ðŸ“‚ Directory Structure

    graphcodebert_project/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ __init__.py           # Package initialization
    â”‚   â”œâ”€â”€ utils.py              # Seed fixing and code preprocessing utilities
    â”‚   â”œâ”€â”€ dataset.py            # CodePairDataset and dataset construction helpers
    â”‚   â”œâ”€â”€ model.py              # GraphCodeBERT classifier wrapper
    â”‚   â””â”€â”€ trainer.py            # Training, validation, and inference routines
    â”‚
    â”œâ”€â”€ train.py                  # Training entry script
    â”œâ”€â”€ inference.py              # Inference entry script
    â”œâ”€â”€ configs/
    â”‚   â”œâ”€â”€ train.yaml            # Training configuration
    â”‚   â””â”€â”€ submit.yaml           # Inference configuration
    â”œâ”€â”€ requirements.txt          # Required Python packages
    â”œâ”€â”€ assets/
    â”‚   â”œâ”€â”€ model1.pt             # Example model weights (placeholder)
    â”‚   â””â”€â”€ model2.pt             # Example model weights (placeholder)
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ train.csv             # Training dataset (example)
    â”‚   â”œâ”€â”€ test.csv              # Test dataset (example)
    â”‚   â””â”€â”€ sample_submission.csv # Example submission format
    â”œâ”€â”€ .gitignore                # Files and directories ignored by Git
    â””â”€â”€ .gitattributes            # Git attributes (e.g., LFS settings)

---

## ðŸš€ Getting Started

### Environment Setup

Python **3.9 or later** is recommended.  
Install required dependencies from the project root:

    pip install -r requirements.txt

---

### Training

After preparing the training dataset, run the following command to train the model.  
All training-related settings can be modified in `configs/train.yaml`.

    python train.py --config configs/train.yaml

---

### Inference

To generate predictions on the test set using trained model weights, run:

    python inference.py --config configs/submit.yaml

The inference script supports loading **one or multiple model checkpoints**, optionally ensembling their predictions, and outputs results in the format defined by `sample_submission.csv`.

---

## Notes

- `assets/model1.pt` and `assets/model2.pt` are provided as placeholders.  
  Replace them with actual trained model weights when running inference.
- `data/train.csv` and `data/test.csv` contain simple example code pairs and labels.  
  For real use cases, replace these files with your own datasets.
- The training and inference pipelines are intentionally kept simple.  
  You can extend them by customizing loss functions, evaluation metrics, or data loaders as needed.
